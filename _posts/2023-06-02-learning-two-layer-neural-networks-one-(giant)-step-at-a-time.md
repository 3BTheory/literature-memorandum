---
layout: post
title: "Learning Two-Layer Neural Networks One (Giant) Step at a Time"
tags: ["deep learning dynamics","feature learning"]
last_modified_at: 2023-06-02 00:41:15 +0900
---

2層NNの表現学習に関する理論研究．大バッチサイズ・少ステップでの表現学習を解析．勾配法1ステップで学習できる部分空間とそのために必要（十分）なバッチサイズをデータ分布のパラメータで特徴づけた．また1ステップごとに1次元ずつ部分空間を学習すると予想．

## 基本情報
   
```bibtex
@misc{dandi2023learning,
      title={Learning Two-Layer Neural Networks, One (Giant) Step at a Time}, 
      author={Yatin Dandi and Florent Krzakala and Bruno Loureiro and Luca Pesce and Ludovic Stephan},
      year={2023},
      eprint={2305.18270},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
```

### 論文リンク

[arXiv](https://arxiv.org/abs/2305.18270)

### 著者・所属

Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, Ludovic Stephan

## 新規性

## 手法

## 結果

## 議論・コメント

## 関連文献

* [Neural Networks can Learn Representations with Gradient Descent](https://proceedings.mlr.press/v178/damian22a.html)
* [High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation](https://proceedings.neurips.cc/paper_files/paper/2022/file/f7e7fabd73b3df96c54a320862afcb78-Paper-Conference.pdf)

