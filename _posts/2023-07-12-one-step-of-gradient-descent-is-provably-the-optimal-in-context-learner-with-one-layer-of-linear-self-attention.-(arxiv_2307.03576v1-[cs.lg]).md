---
layout: post
title: "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. (arXiv:2307.03576v1 [cs.LG])"
tags: ["foundation models","language","in-context learning"]
last_modified_at: 2023-07-12 00:27:08 +0900
---

一層の線形自己注意NNにおいて，in-context learningが1ステップの勾配降下を実現していることを示した．入力データの共分散は勾配法の前処理行列に影響する一方，ターゲットの非線形性はアルゴリズムを変えないことを示した．Ahn et al 2023やZhang et al 2023も参照．

## 基本情報

```bibtex
@misc{mahankali2023step,
      title={One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention}, 
      author={Arvind Mahankali and Tatsunori B. Hashimoto and Tengyu Ma},
      year={2023},
      eprint={2307.03576},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

### 論文リンク

[arXiv](https://arxiv.org/abs/2307.03576)

### 著者・所属

* Arvind Mahankali, Tatsunori B. Hashimoto, Tengyu Ma (Stanford)

## 新規性

## 手法

## 結果

## 議論・コメント

## 関連文献

* Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning, 2023.
* Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023.
